<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Data Derp RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Data Derp Atom Feed"><title data-react-helmet="true">Spark Workflow and Partitioning | Data Derp</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://data-derp.github.io/docs/data-transformation/spark-workflow-and-partitioning"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Spark Workflow and Partitioning | Data Derp"><meta data-react-helmet="true" name="description" content="Optimisation"><meta data-react-helmet="true" property="og:description" content="Optimisation"><link data-react-helmet="true" rel="canonical" href="https://data-derp.github.io/docs/data-transformation/spark-workflow-and-partitioning"><link data-react-helmet="true" rel="alternate" href="https://data-derp.github.io/docs/data-transformation/spark-workflow-and-partitioning" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://data-derp.github.io/docs/data-transformation/spark-workflow-and-partitioning" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.9a716cea.css">
<link rel="preload" href="/assets/js/runtime~main.c53c3195.js" as="script">
<link rel="preload" href="/assets/js/main.c8dc4f9d.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title">Data Derp</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Lessons</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/data-derp/data-derp.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">🌜</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">🌞</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tour Agenda</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/intro-to-data-engineering/what-is-data-engineering">Intro to Data Engineering</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/data-engineering-the-good-parts/data-milky-way-brief-history-part-1">Data Engineering: The Good Parts</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/project-design/overview">Project Design</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/data-ingestion/exercise-core-logic">Data Ingestion</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_TwRn" href="/docs/data-transformation/overview">Data Transformation</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/overview">Overview</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/why-transform-data">Why Transform Data?</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/etl-vs-elt">ETL vs. ELT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/sql-quick-review">SQL: A Quick Review</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/apache-spark">Apache Spark</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/exercise-wrangling-with-spark">Exercise: Wrangling with Spark</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/data-transformation/spark-workflow-and-partitioning">Spark Workflow and Partitioning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/advanced-wrangling">Advanced Wrangling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/exercise-core-logic">Exercise: Core Logic</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/exercise-workflow">Exercise: Create a Workflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/exercise-query-engine">Exercise: Query your Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/quiz">Quiz</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/spark-advanced-topics">Spark Advanced Topics (Optional)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/airflow">Airflow (Optional)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/data-transformation/spark-on-kubernetes">Spark on Kubernetes (Optional)</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/data-visualisation/understanding-your-data">Data Visualisation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/data-science-and-interpretability/overview">Data Science and Interpretability</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/data-security/overview">Data Security</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/beyond-the-batch/overview">Beyond the Batch</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/making-big-data-work/overview">Making Big Data Work</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" href="/docs/data-mesh/intro">Data Mesh</a></div></li></ul></nav></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Spark Workflow and Partitioning</h1></header><div style="text-align:center"><figure class="video-container"><iframe width="560" height="315" src="https://www.youtube.com/embed/rx-J34dKYUc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"></iframe></figure></div><h2 class="anchor anchorWithStickyNavbar_y2LR" id="optimisation">Optimisation<a class="hash-link" href="#optimisation" title="Direct link to heading">​</a></h2><p>Okay...I know how to wrangle/transform my data...but how do I actually optimize my job’s performance?
Golden Rule: In the real world, make sure your dataset/table is partitioned well</p><ul><li>Lots of small files are the enemy!<ul><li>Having lots of tiny files will result in S3 needing to do lots of <strong>file listing</strong> operations. These are extremely slow and can even be expensive</li><li>Lots of small files means lots of data shuffling through the network. <strong>This is slow!</strong></li></ul></li><li><strong>HUGE</strong> files are also bad<ul><li>Having too few files (all being huge) means you probably won’t take advantage of all of the cores in your cluster. In other words, the data can’t be easily distributed around the cluster</li><li>Each node in your cluster might even have to try and break down each of these huge files in order to redistribute some data to other nodes. This is a waste of time and money (<a href="https://www.youtube.com/watch?v=982wFqC03v8&amp;ab_channel=pyromaniack" target="_blank" rel="noopener noreferrer">must-watch</a>)</li></ul></li><li>So what’s a suitable strategy?<ul><li>There’s no ‘best’ number. Try to target each .snappy.parquet file to be somewhere <strong>roughly between 256MB to 1GB</strong></li><li>More importantly, make sure that you’re partitioning on columns that you frequently <strong>filter</strong> or do <strong>groupBy</strong> on</li><li><strong>DO NOT</strong> partition on columns with high cardinality (e.g. a userId, which has millions of distinct values)
this will result in lots of <strong>small files and lots of file listing operations</strong></li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="partitioning">Partitioning<a class="hash-link" href="#partitioning" title="Direct link to heading">​</a></h2><div style="text-align:center"><p><img alt="partitioning.png" src="/assets/images/partitioning-ac6b6c56d48894919e4f9058a0a8ab9c.png"></p></div><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</h5></div><div class="admonition-content"><p>Partitioning strategy is the most important decision we have to get right!</p></div></div><p>If your partitioning strategy is decent, you’ll most likely be fine and won’t need to tweak other knobs.
Especially going forward in the future with Spark 3.0’s <a href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html" target="_blank" rel="noopener noreferrer">Adaptive Query Execution (AQE)</a>, a lot of optimizations will be automated for you!</p><p>So how does a partitioned table look?</p><ul><li>It would actually look like a bunch of hierarchical folders</li><li>The partitioning values become their own folder (e.g. year=2018)</li><li>The underlying data will be at the bottom of the hierarchy and will</li><li>often have a .snappy.parquet file extension (if using Spark)</li></ul><p>Can you give me an example?</p><ul><li>Partitioning the table based on some notion of time is a popular option
(check if that makes sense for your project first though!)</li><li>e.g. assuming each day of data for the table is of the order of 128MB - 1GB, then
your partitioning keys can be (“year”, “month”, “day”)</li><li>You don’t need to explicitly define all the values, Spark will smartly
create a new partition for each distinct combination of your partitioning values</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="working-with-partitioned-data">Working with Partitioned Data<a class="hash-link" href="#working-with-partitioned-data" title="Direct link to heading">​</a></h2><div style="text-align:center"><figure class="video-container"><iframe width="560" height="315" src="https://www.youtube.com/embed/fhEJG2oFCm8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"></iframe></figure></div><h2 class="anchor anchorWithStickyNavbar_y2LR" id="partitioning-faqs">Partitioning FAQs<a class="hash-link" href="#partitioning-faqs" title="Direct link to heading">​</a></h2><div style="text-align:center"><p><img alt="partitioning.png" src="/assets/images/partitioning-ac6b6c56d48894919e4f9058a0a8ab9c.png"></p></div><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</h5></div><div class="admonition-content"><p> Question: So...is a parquet file a file or a folder of files?</p></div></div><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</h5></div><div class="admonition-content"><p>Short Answer: either!</p></div></div><ul><li>With a single-node library like Pandas, you can write a single .snappy.parquet file if you want</li><li>However, in the real-world they are often folders of partitions<ul><li>This way you can read/write an entire table with just one path (the root of the table)<ul><li>e.g. s3://my-bucket/my-table/ or s3://my-bucket/my-table.parquet/ (both of these styles are still folders)</li><li>Underneath all of the partitioning folders, you will find your .snappy.parquet files</li></ul></li><li>The query engine (e.g. Spark or Presto) will then take care of understanding the partitioning structure of the table and will optimize your queries around that</li><li>Spark will always write the output of a DataFrame as a folder at the root level rather than a single file (because it’s designed for distributed/concurrent reading/writing of data, which often involves multiple files)</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="shuffling">Shuffling<a class="hash-link" href="#shuffling" title="Direct link to heading">​</a></h2><div style="text-align:center"><p><img alt="shuffling.png" src="/assets/images/shuffling-ed3cfadd55c02ea6dddadb4cb0f42f96.png"></p><p><a href="https://blog.scottlogic.com/2018/03/22/apache-spark-performance.html#:~:text=A%20shuffle%20occurs%20when%20data,likely%20on%20a%20different%20executor." target="_blank" rel="noopener noreferrer">Reference</a></p></div><h2 class="anchor anchorWithStickyNavbar_y2LR" id="resources">Resources<a class="hash-link" href="#resources" title="Direct link to heading">​</a></h2><div style="text-align:center"><p><img alt="fear-path-to-dark-side.png" src="/assets/images/fear-path-to-dark-side-02df5aba18d3a6b7209b559ad8883194.png"></p></div><ul><li><a href="https://blog.rockthejvm.com/spark-dags/" target="_blank" rel="noopener noreferrer">Spark DAGs and planning</a> (optional)<ul><li>Just know that bad partitioning → shuffling → pain (must-watch)</li><li>You can check how ‘shuffly’ your Spark job looks by viewing the DAG</li></ul></li><li><a href="https://mungingdata.com/apache-spark/partitionby/" target="_blank" rel="noopener noreferrer">Managing Partitioning</a><ul><li>Important: understand that repartition() and DataFrame.write.partitionBy() are <strong>not</strong> the same thing<ul><li>Repartition can take in 2 different types of arguments:<ul><li>a number: controls the number of .snappy.parquet files</li><li>a bunch of column names: it will ensure 1 .snappy.parquet file per each distinct combination of your provided columns</li></ul></li><li>DataFrame.write.partitionBy defines the folder structure of the table <ul><li>however, it does not guarantee how many .snappy.parquet files will be in each folder </li></ul></li><li>Sometimes you might even need to do both e.g. <code>df.repartition(“year”, “month”).write.partitionBy(“year”, ”month”)...</code> in order to guarantee exactly 1 .snappy.parquet file per each month folder</li></ul></li><li>Try to read up on the difference between repartition and coalesce <ul><li>Short Answer: <a href="https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce" target="_blank" rel="noopener noreferrer">The coalesce transformation applied to a DataFrame</a> (not to be confused with <a href="https://spark.apache.org/docs/latest/api/python//reference/pyspark.sql/api/pyspark.sql.functions.coalesce.html" target="_blank" rel="noopener noreferrer">coalesce() applied to a column</a>), will try to merge partitions to reach your desired number. You only use coalesce when you want to reduce the number of partitions in your data.</li><li>On the other hand, repartition() will full shuffle all of the data around (more expensive).</li><li>If you need to increase the number of partitions in your data, then you will need repartition()</li></ul></li></ul></li><li><a href="https://github.com/data-derp/small-exercises/blob/master/databricks-repartition-vs-write-partition-by.dbc" target="_blank" rel="noopener noreferrer">Practice Repartitioning vs PartitionBy in DataBricks</a></li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/data-derp/data-derp.github.io/tree/master/docs/docs/data-transformation/spark-workflow-and-partitioning.mdx" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mS5F" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_mt2f"><span class="theme-last-updated">Last updated<!-- --> by <b>Kelsey Mok</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/data-transformation/exercise-wrangling-with-spark"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Exercise: Wrangling with Spark</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/data-transformation/advanced-wrangling"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Advanced Wrangling</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#optimisation" class="table-of-contents__link toc-highlight">Optimisation</a></li><li><a href="#partitioning" class="table-of-contents__link toc-highlight">Partitioning</a></li><li><a href="#working-with-partitioned-data" class="table-of-contents__link toc-highlight">Working with Partitioned Data</a></li><li><a href="#partitioning-faqs" class="table-of-contents__link toc-highlight">Partitioning FAQs</a></li><li><a href="#shuffling" class="table-of-contents__link toc-highlight">Shuffling</a></li><li><a href="#resources" class="table-of-contents__link toc-highlight">Resources</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Lessons</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="#" class="footer__link-item">Slack</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/data-derp" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 Data Derp</div></div></div></footer></div>
<script src="/assets/js/runtime~main.c53c3195.js"></script>
<script src="/assets/js/main.c8dc4f9d.js"></script>
</body>
</html>